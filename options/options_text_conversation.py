from transformers.feature_extraction_utils import PreTrainedFeatureExtractor
from transformers.image_processing_utils import BaseImageProcessor

from options.options import Options, Devices
import torch
from typing import Optional, Union, List, Dict, Tuple, Any, Callable
from transformers import pipeline, PreTrainedModel, TFPreTrainedModel, PretrainedConfig, PreTrainedTokenizer, \
    PreTrainedTokenizerFast, ModelCard


class OptionsTextConversation(Options):
    """
    Options for text-Generation models
    """
    """
    Args:
        device Union[str, Devices]:
            The device to use for inference, specified from the `Devices` enum.
        
        prompt (str, optional):
            The prompt or starting text for text generation. Default is an empty string.
        
        model (Union[str, PreTrainedModel, "TFPreTrainedModel"], optional):
            The model to use for text generation. This can be a model identifier, a pre-trained model instance
            inheriting from `PreTrainedModel` for PyTorch, or `"TFPreTrainedModel"` for TensorFlow.
        
        tokenizer (Union[str, PreTrainedTokenizer, "PreTrainedTokenizerFast"], optional):
            The tokenizer to use for tokenizing input text. This can be a model identifier, a pre-trained tokenizer
            instance inheriting from `PreTrainedTokenizer`, or `"PreTrainedTokenizerFast"`.
        
        model_card (Optional[Union[str, ModelCard]], optional):
            The model card providing details about the model, such as usage guidelines and citations.
        
        framework (Optional[str], optional):
            The deep learning framework used for the model, such as 'torch' or 'tf'.
        
        task (str, optional):
            The task for which the model is being used. Default is an empty string.
        
        num_workers (Optional[int], optional):
            The number of worker processes for data loading. Default is 8.
        
        batch_size (Optional[int], optional):
            The batch size for inference. Default is 1.

        arg_parser (Optional[Dict[str, Any]], optional):
            Optional additional arguments for the model.
        
        torch_dtype (Optional[Union[str, torch.dtype]], optional):
            The data type for PyTorch tensors, such as 'float32' or 'float64'.
        
        binary_output (Optional[bool], optional):
            Whether the output should be binary or text. Default is False.
        
        min_length_for_response (Optional[int], optional):
            The minimum length of response generated by the model. Default is 32.
        
        minimum_tokens (Optional[int], optional):
            The minimum number of tokens required for a valid response. Default is 10.
        
        max_steps (Optional[int], optional):
            The maximum number of steps for generating text. Default is 50.

        """

    prompt: str
    model: Union[str, PreTrainedModel, "TFPreTrainedModel"] = None
    tokenizer: Union[str, PreTrainedTokenizer, "PreTrainedTokenizerFast"] = None
    model_card: Optional[Union[str, ModelCard]] = None
    framework: Optional[str] = None
    task: str = ""
    num_workers: int = 8
    batch_size: int = 1
    device: Union[str, Devices] = -1
    arg_parser: Optional[Dict[str, Any]] = None
    torch_dtype: Optional[Union[str, torch.dtype]] = None
    binary_output: bool = False
    min_length_for_response: int = 32
    minimum_tokens: int = 10
    max_steps: int = 50

    def __init__(self,
                 prompt: str,
                 model: Union[str, PreTrainedModel, "TFPreTrainedModel"] = None,
                 tokenizer: Union[str, PreTrainedTokenizer, "PreTrainedTokenizerFast"] = None,
                 model_card: Optional[Union[str, ModelCard]] = None,
                 framework: Optional[str] = None,
                 task: str = "",
                 num_workers: int = 8,
                 batch_size: int = 1,
                 device: Union[str, Devices] = -1,
                 arg_parser: Optional[Dict[str, Any]] = None,
                 torch_dtype: Optional[Union[str, torch.dtype]] = None,
                 binary_output: bool = False,
                 min_length_for_response: int = 32,
                 minimum_tokens: int = 10,
                 max_steps: int = 50
                 ):
        """
        Initializes the OptionsTextGeneration.
        """
        super().__init__(device)
        # Initialize additional attributes
        self.prompt = prompt
        self.task = task
        self.model = model
        self.tokenizer = tokenizer
        if model_card:
            self.model_card = model_card
        self.num_workers = num_workers
        self.batch_size = batch_size
        if framework:
            self.framework = framework
        if arg_parser:
            self.arg_parser = arg_parser
        if torch_dtype:
            self.torch_dtype = torch_dtype
        self.torch_dtype = torch_dtype
        self.binary_output = binary_output
        self.min_length_for_response = min_length_for_response
        self.minimum_tokens = minimum_tokens
        self.max_steps = max_steps
